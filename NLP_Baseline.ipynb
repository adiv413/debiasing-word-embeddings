{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUX5jdJRHAEj"
      },
      "outputs": [],
      "source": [
        "# https://petamind.com/word2vec-with-tensorflow-2-0-a-simple-cbow-implementation/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxLCwOd9rmMw",
        "outputId": "8c6cb483-6647-4511-efcb-f59dcd1f4102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RbQbEvMHhkl",
        "outputId": "f24e8bf7-895e-4ed1-9459-e2d4499a83d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "print(tf.__version__)\n",
        "##Output\n",
        "#TensorFlow 2.x selected.\n",
        "#2.0.0-rc2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwKbZttyuA5B"
      },
      "outputs": [],
      "source": [
        "class Word2Vec:\n",
        "  def __init__(self, vocab_size=0, embedding_dim=16, optimizer='adam', epochs=1):\n",
        "    self.vocab_size=vocab_size\n",
        "    self.embedding_dim=5\n",
        "    self.epochs=epochs\n",
        "    if optimizer=='adam':\n",
        "      self.optimizer = tf.optimizers.Adam()\n",
        "    else:\n",
        "      self.optimizer = tf.optimizers.SGD(learning_rate=1)\n",
        "\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.vocab_size, self.embedding_dim]))\n",
        "    self.b1 = tf.Variable(tf.random.normal([self.embedding_dim])) #bias\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.embedding_dim, self.vocab_size]))\n",
        "    self.b2 = tf.Variable(tf.random.normal([self.vocab_size]))\n",
        "\n",
        "    # self.checkpoint = tf.train.Checkpoint()\n",
        "  \n",
        "  def train(self, x_train=None, y_train=None):\n",
        "    for _ in range(1, self.epochs + 1):\n",
        "      with tf.GradientTape() as t:\n",
        "        #print(x_train, self.W1)\n",
        "        hidden_layer = tf.add(tf.matmul(x_train,self.W1),self.b1)\n",
        "        output_layer = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, self.W2), self.b2))\n",
        "        cross_entropy_loss = tf.reduce_mean(-tf.math.reduce_sum(y_train * tf.math.log(output_layer), axis=[1]))\n",
        "      grads = t.gradient(cross_entropy_loss, [self.W1, self.b1, self.W2, self.b2])\n",
        "      self.optimizer.apply_gradients(zip(grads,[self.W1, self.b1, self.W2, self.b2]))\n",
        "      if(_ % (self.epochs - 2) == 0):\n",
        "        print(cross_entropy_loss)\n",
        "  \n",
        "  def vectorized(self, word_idx):\n",
        "    return (self.W1+self.b1)[word_idx]\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_one_hot(data_points, vocab_size):\n",
        "  temp = np.zeros(vocab_size)\n",
        "  for i in data_points:\n",
        "    temp[i] = 1\n",
        "  return temp\n",
        "\n",
        "stopwords = {'a', 'about', 'above', 'across', 'after', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'among', 'an', 'and', 'another', 'any', 'anybody', 'anyone', 'anything', 'anywhere', 'are', 'area', 'areas', 'around', 'as', 'ask', 'asked', 'asking', 'asks', 'at', 'away', 'b', 'back', 'backed', 'backing', 'backs', 'be', 'became', 'because', 'become', 'becomes', 'been', 'before', 'began', 'behind', 'being', 'beings', 'best', 'better', 'between', 'big', 'both', 'but', 'by', 'c', 'came', 'can', 'cannot', 'case', 'cases', 'certain', 'certainly', 'clear', 'clearly', 'come', 'could', 'd', 'did', 'differ', 'different', 'differently', 'do', 'does', 'done', 'down', 'down', 'downed', 'downing', 'downs', 'during', 'e', 'each', 'early', 'either', 'end', 'ended', 'ending', 'ends', 'enough', 'even', 'evenly', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'f', 'face', 'faces', 'fact', 'facts', 'far', 'felt', 'few', 'find', 'finds', 'first', 'for', 'four', 'from', 'full', 'fully', 'further', 'furthered', 'furthering', 'furthers', 'g', 'gave', 'general', 'generally', 'get', 'gets', 'give', 'given', 'gives', 'go', 'going', 'good', 'goods', 'got', 'great', 'greater', 'greatest', 'group', 'grouped', 'grouping', 'groups', 'h', 'had', 'has', 'have', 'having', 'he', 'her', 'here', 'herself', 'high', 'high', 'high', 'higher', 'highest', 'him', 'himself', 'his', 'how', 'however', 'i', 'if', 'important', 'in', 'interest', 'interested', 'interesting', 'interests', 'into', 'is', 'it', 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kind', 'knew', 'know', 'known', 'knows', 'l', 'large', 'largely', 'last', 'later', 'latest', 'least', 'less', 'let', 'lets', 'like', 'likely', 'long', 'longer', 'longest', 'm', 'made', 'make', 'making', 'man', 'many', 'may', 'me', 'member', 'members', 'men', 'might', 'more', 'most', 'mostly', 'mr', 'mrs', 'much', 'must', 'my', 'myself', 'n', 'necessary', 'need', 'needed', 'needing', 'needs', 'never', 'new', 'new', 'newer', 'newest', 'next', 'no', 'nobody', 'non', 'noone', 'not', 'nothing', 'now', 'nowhere', 'number', 'numbers', 'o', 'of', 'off', 'often', 'old', 'older', 'oldest', 'on', 'once', 'one', 'only', 'open', 'opened', 'opening', 'opens', 'or', 'order', 'ordered', 'ordering', 'orders', 'other', 'others', 'our', 'out', 'over', 'p', 'part', 'parted', 'parting', 'parts', 'per', 'perhaps', 'place', 'places', 'point', 'pointed', 'pointing', 'points', 'possible', 'present', 'presented', 'presenting', 'presents', 'problem', 'problems', 'put', 'puts', 'q', 'quite', 'r', 'rather', 'really', 'right', 'right', 'room', 'rooms', 's', 'said', 'same', 'saw', 'say', 'says', 'second', 'seconds', 'see', 'seem', 'seemed', 'seeming', 'seems', 'sees', 'several', 'shall', 'she', 'should', 'show', 'showed', 'showing', 'shows', 'side', 'sides', 'since', 'small', 'smaller', 'smallest', 'so', 'some', 'somebody', 'someone', 'something', 'somewhere', 'state', 'states', 'still', 'still', 'such', 'sure', 't', 'take', 'taken', 'than', 'that', 'the', 'their', 'them', 'then', 'there', 'therefore', 'these', 'they', 'thing', 'things', 'think', 'thinks', 'this', 'those', 'though', 'thought', 'thoughts', 'three', 'through', 'thus', 'to', 'today', 'together', 'too', 'took', 'toward', 'turn', 'turned', 'turning', 'turns', 'two', 'u', 'under', 'until', 'up', 'upon', 'us', 'use', 'used', 'uses', 'v', 'very', 'w', 'want', 'wanted', 'wanting', 'wants', 'was', 'way', 'ways', 'we', 'well', 'wells', 'went', 'were', 'what', 'when', 'where', 'whether', 'which', 'while', 'who', 'whole', 'whose', 'why', 'will', 'with', 'within', 'without', 'work', 'worked', 'working', 'works', 'would', 'x', 'y', 'year', 'years', 'yet', 'you', 'young', 'younger', 'youngest', 'your', 'yours', 'z'}"
      ],
      "metadata": {
        "id": "UWiSn-SGy0AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "from math import ceil\n",
        "\n",
        "count = 0\n",
        "max_num_lines = 1\n",
        "words = set()\n",
        "\n",
        "with open(\"drive/MyDrive/wiki_corpus.txt\", encoding=\"utf-8\", errors=\"ignore\") as infile:\n",
        "    for line in infile:\n",
        "      if count >= max_num_lines:\n",
        "          break\n",
        "\n",
        "      corpus_raw = line.replace(\"'\", \"\")\n",
        "      corpus_raw = corpus_raw.replace('\"', \"\")\n",
        "      pattern = re.compile('[\\W_-–]+')\n",
        "      pattern.sub('', corpus_raw)\n",
        "\n",
        "      # print(corpus_raw)\n",
        "\n",
        "      # convert to lower case\n",
        "      corpus_raw = corpus_raw.lower()\n",
        "      # raw sentences is a list of sentences.\n",
        "      raw_sentences_no_stopword_filter = corpus_raw.split('.')\n",
        "\n",
        "      processed_sentences = []\n",
        "      for phrase in raw_sentences_no_stopword_filter:\n",
        "        new_phrase = [word for word in phrase.split() if word not in stopwords]\n",
        "        processed_sentences.append(new_phrase)  \n",
        "\n",
        "      \n",
        "      for sentence in processed_sentences:\n",
        "          for word in sentence:\n",
        "              if word and word not in words:\n",
        "                words.add(word)\n",
        "                \n",
        "      count += 1"
      ],
      "metadata": {
        "id": "0_HzDDLzFd0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjPbxrvzCnpR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "599c9492-2ae9-4496-ccc8-fdb90c8d3dbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1871\n",
            "tf.Tensor(4.35562, shape=(), dtype=float32)\n",
            "tf.Tensor(2.7366457, shape=(), dtype=float32)\n",
            "tf.Tensor(1.1620494, shape=(), dtype=float32)\n",
            "tf.Tensor(0.63328046, shape=(), dtype=float32)\n",
            "tf.Tensor(0.4480574, shape=(), dtype=float32)\n",
            "tf.Tensor(0.4281042, shape=(), dtype=float32)\n",
            "tf.Tensor(0.52185434, shape=(), dtype=float32)\n",
            "tf.Tensor(0.43936697, shape=(), dtype=float32)\n",
            "tf.Tensor(0.26617762, shape=(), dtype=float32)\n",
            "tf.Tensor(0.27060577, shape=(), dtype=float32)\n",
            "tf.Tensor(0.5122725, shape=(), dtype=float32)\n",
            "tf.Tensor(0.7581588, shape=(), dtype=float32)\n",
            "tf.Tensor(0.20726107, shape=(), dtype=float32)\n",
            "tf.Tensor(0.25092572, shape=(), dtype=float32)\n",
            "tf.Tensor(0.6181364, shape=(), dtype=float32)\n",
            "tf.Tensor(0.44059995, shape=(), dtype=float32)\n",
            "tf.Tensor(0.26485294, shape=(), dtype=float32)\n",
            "tf.Tensor(0.3035458, shape=(), dtype=float32)\n",
            "tf.Tensor(0.2285624, shape=(), dtype=float32)\n",
            "tf.Tensor(0.3174403, shape=(), dtype=float32)\n",
            "tf.Tensor(0.215814, shape=(), dtype=float32)\n",
            "tf.Tensor(0.41471368, shape=(), dtype=float32)\n",
            "tf.Tensor(0.30689847, shape=(), dtype=float32)\n",
            "tf.Tensor(0.4389953, shape=(), dtype=float32)\n",
            "tf.Tensor(0.16317621, shape=(), dtype=float32)\n",
            "tf.Tensor(0.15804815, shape=(), dtype=float32)\n",
            "tf.Tensor(0.18921173, shape=(), dtype=float32)\n",
            "tf.Tensor(0.4441661, shape=(), dtype=float32)\n",
            "tf.Tensor(0.36490837, shape=(), dtype=float32)\n",
            "tf.Tensor(0.627557, shape=(), dtype=float32)\n",
            "tf.Tensor(0.854475, shape=(), dtype=float32)\n",
            "tf.Tensor(0.6093137, shape=(), dtype=float32)\n",
            "tf.Tensor(0.18235098, shape=(), dtype=float32)\n",
            "tf.Tensor(0.9099074, shape=(), dtype=float32)\n",
            "tf.Tensor(0.9244014, shape=(), dtype=float32)\n",
            "tf.Tensor(0.17359856, shape=(), dtype=float32)\n",
            "tf.Tensor(0.15525506, shape=(), dtype=float32)\n",
            "tf.Tensor(0.17600572, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0902948, shape=(), dtype=float32)\n",
            "tf.Tensor(0.29374158, shape=(), dtype=float32)\n",
            "tf.Tensor(0.5374537, shape=(), dtype=float32)\n",
            "tf.Tensor(0.39467165, shape=(), dtype=float32)\n",
            "tf.Tensor(0.2737544, shape=(), dtype=float32)\n",
            "tf.Tensor(0.33046806, shape=(), dtype=float32)\n",
            "tf.Tensor(0.19363658, shape=(), dtype=float32)\n",
            "tf.Tensor(0.21429694, shape=(), dtype=float32)\n",
            "tf.Tensor(0.382181, shape=(), dtype=float32)\n",
            "tf.Tensor(0.19747566, shape=(), dtype=float32)\n",
            "tf.Tensor(0.40262124, shape=(), dtype=float32)\n",
            "tf.Tensor(0.14004408, shape=(), dtype=float32)\n",
            "tf.Tensor(0.13972071, shape=(), dtype=float32)\n",
            "tf.Tensor(0.2569321, shape=(), dtype=float32)\n",
            "tf.Tensor(0.39341635, shape=(), dtype=float32)\n",
            "tf.Tensor(0.16580862, shape=(), dtype=float32)\n",
            "tf.Tensor(0.28515944, shape=(), dtype=float32)\n",
            "tf.Tensor(0.32446438, shape=(), dtype=float32)\n",
            "tf.Tensor(0.5695767, shape=(), dtype=float32)\n",
            "tf.Tensor(0.40637124, shape=(), dtype=float32)\n",
            "tf.Tensor(0.3764136, shape=(), dtype=float32)\n",
            "tf.Tensor(0.38673794, shape=(), dtype=float32)\n",
            "tf.Tensor(0.23545389, shape=(), dtype=float32)\n",
            "tf.Tensor(0.36933962, shape=(), dtype=float32)\n",
            "tf.Tensor(0.5061223, shape=(), dtype=float32)\n",
            "tf.Tensor(0.07411164, shape=(), dtype=float32)\n",
            "tf.Tensor(0.19243848, shape=(), dtype=float32)\n",
            "tf.Tensor(0.3807432, shape=(), dtype=float32)\n",
            "tf.Tensor(0.4165728, shape=(), dtype=float32)\n",
            "tf.Tensor(0.17902783, shape=(), dtype=float32)\n",
            "tf.Tensor(0.40200245, shape=(), dtype=float32)\n",
            "tf.Tensor(0.2805279, shape=(), dtype=float32)\n",
            "tf.Tensor(0.14672875, shape=(), dtype=float32)\n",
            "tf.Tensor(0.19322985, shape=(), dtype=float32)\n",
            "tf.Tensor(0.3389974, shape=(), dtype=float32)\n",
            "tf.Tensor(0.29285592, shape=(), dtype=float32)\n",
            "tf.Tensor(0.179423, shape=(), dtype=float32)\n",
            "tf.Tensor(0.29786798, shape=(), dtype=float32)\n",
            "tf.Tensor(0.21577282, shape=(), dtype=float32)\n",
            "tf.Tensor(0.11649424, shape=(), dtype=float32)\n",
            "tf.Tensor(0.10299585, shape=(), dtype=float32)\n",
            "tf.Tensor(0.2304197, shape=(), dtype=float32)\n",
            "tf.Tensor(0.24576908, shape=(), dtype=float32)\n",
            "tf.Tensor(0.23707177, shape=(), dtype=float32)\n",
            "tf.Tensor(0.27200985, shape=(), dtype=float32)\n",
            "tf.Tensor(0.26763007, shape=(), dtype=float32)\n",
            "tf.Tensor(0.47324237, shape=(), dtype=float32)\n",
            "tf.Tensor(0.1956635, shape=(), dtype=float32)\n",
            "tf.Tensor(0.14210202, shape=(), dtype=float32)\n",
            "tf.Tensor(0.32282352, shape=(), dtype=float32)\n",
            "tf.Tensor(0.15252003, shape=(), dtype=float32)\n",
            "tf.Tensor(0.12565406, shape=(), dtype=float32)\n",
            "tf.Tensor(0.19141418, shape=(), dtype=float32)\n",
            "tf.Tensor(0.16497205, shape=(), dtype=float32)\n",
            "tf.Tensor(0.17902781, shape=(), dtype=float32)\n",
            "tf.Tensor(0.18197446, shape=(), dtype=float32)\n",
            "tf.Tensor(0.12676328, shape=(), dtype=float32)\n",
            "tf.Tensor(0.400941, shape=(), dtype=float32)\n",
            "tf.Tensor(0.51510966, shape=(), dtype=float32)\n",
            "tf.Tensor(0.31346843, shape=(), dtype=float32)\n",
            "tf.Tensor(0.25680047, shape=(), dtype=float32)\n",
            "tf.Tensor(0.18143737, shape=(), dtype=float32)\n",
            "0.004640340805053711 0.018668413162231445 248.81795859336853\n"
          ]
        }
      ],
      "source": [
        "lines = []\n",
        "count = 0\n",
        "# vocab_size = 30039    # actual vocab size\n",
        "vocab_size = len(words) + 1\n",
        "word2int = {}\n",
        "int2word = {}\n",
        "WINDOW_SIZE = 5\n",
        "\n",
        "t = 0\n",
        "t1 = 0\n",
        "t2 = 0\n",
        "print(vocab_size)\n",
        "cbow = Word2Vec(vocab_size=vocab_size, optimizer='adam', epochs=500)\n",
        "\n",
        "with open(\"drive/MyDrive/wiki_corpus.txt\", encoding=\"utf-8\", errors=\"ignore\") as infile:\n",
        "    for line in infile:\n",
        "      s = time.time()\n",
        "      if count >= max_num_lines:\n",
        "          break\n",
        "\n",
        "      corpus_raw = line.replace(\"'\", \"\")\n",
        "      corpus_raw = corpus_raw.replace('\"', \"\")\n",
        "      pattern = re.compile('[\\W_-–]+')\n",
        "      pattern.sub('', corpus_raw)\n",
        "\n",
        "      # print(corpus_raw)\n",
        "\n",
        "      # convert to lower case\n",
        "      corpus_raw = corpus_raw.lower()\n",
        "      # raw sentences is a list of sentences.\n",
        "      raw_sentences_no_stopword_filter = corpus_raw.split('.')\n",
        "\n",
        "      processed_sentences = []\n",
        "      for phrase in raw_sentences_no_stopword_filter:\n",
        "        new_phrase = [word for word in phrase.split() if word not in stopwords]\n",
        "        processed_sentences.append(new_phrase)  \n",
        "\n",
        "      for sentence in processed_sentences:\n",
        "          for word in sentence:\n",
        "              if word and word not in word2int:\n",
        "                word2int[word] = len(word2int)\n",
        "                int2word[len(int2word)] = word\n",
        "\n",
        "      if len(word2int) >= vocab_size:\n",
        "        break\n",
        "\n",
        "      t += time.time() - s\n",
        "      for sentence in processed_sentences[:100]:\n",
        "        s = time.time()\n",
        "        # print(sentence)\n",
        "\n",
        "        #sentences:\n",
        "        data = []\n",
        "        for word_index, word in enumerate(sentence):\n",
        "            prediction_words = [word2int[i] for i in sentence[max(0, word_index - int(WINDOW_SIZE/2)) : min(len(sentence), word_index + ceil(WINDOW_SIZE/2))] if i != word]\n",
        "            data.append([prediction_words, [word2int[word]]])\n",
        "        \n",
        "        x_train = [] # input word\n",
        "        y_train = [] # output word\n",
        "\n",
        "        for data_word in data:\n",
        "            x_train.append(to_one_hot(data_word[0], vocab_size))\n",
        "            y_train.append(to_one_hot(data_word[1], vocab_size))\n",
        "\n",
        "        # convert them to numpy arrays\n",
        "        x_train = np.asarray(x_train, dtype='float32')\n",
        "        y_train = np.asarray(y_train, dtype='float32')\n",
        "\n",
        "        t1 += time.time() - s\n",
        "\n",
        "        try:\n",
        "          s = time.time()\n",
        "          cbow.train(x_train, y_train)\n",
        "          t2 += time.time() - s\n",
        "        except:\n",
        "          continue\n",
        "\n",
        "      count += 1\n",
        "\n",
        "print(t, t1, t2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3sezRA8Ddrt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e637cc0-4d17-4b4e-d45e-373d6708fc38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0.42489704, shape=(), dtype=float32)\n",
            "tf.Tensor([ 1.2133877   2.879199    0.3810059  -0.07465483  1.2235802 ], shape=(5,), dtype=float32)\n",
            "tf.Tensor([3.337437   0.02535027 1.0269549  1.2398603  0.8446161 ], shape=(5,), dtype=float32)\n",
            "tf.Tensor(14.94485, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "criminal_vector = cbow.vectorized(word2int['black'])\n",
        "black_vector = cbow.vectorized(word2int['white'])\n",
        "\n",
        "normalize_a = tf.nn.l2_normalize(criminal_vector,0)        \n",
        "normalize_b = tf.nn.l2_normalize(black_vector,0)\n",
        "cos_similarity=tf.reduce_sum(tf.multiply(normalize_a,normalize_b))\n",
        "print(cos_similarity)\n",
        "print(criminal_vector)\n",
        "print(black_vector)\n",
        "dist = tf.reduce_sum(tf.square(criminal_vector-black_vector))\n",
        "print(dist)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NLP_Baseline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}