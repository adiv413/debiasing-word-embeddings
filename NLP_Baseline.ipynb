{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUX5jdJRHAEj"
      },
      "outputs": [],
      "source": [
        "# https://petamind.com/word2vec-with-tensorflow-2-0-a-simple-cbow-implementation/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RbQbEvMHhkl",
        "outputId": "48ba99ba-9659-4636-f024-65972bb97e0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "print(tf.__version__)\n",
        "##Output\n",
        "#TensorFlow 2.x selected.\n",
        "#2.0.0-rc2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EwKbZttyuA5B"
      },
      "outputs": [],
      "source": [
        "class Word2Vec:\n",
        "  def __init__(self, vocab_size=0, embedding_dim=16, optimizer='sgd', epochs=10000):\n",
        "    self.vocab_size=vocab_size\n",
        "    self.embedding_dim=5\n",
        "    self.epochs=epochs\n",
        "    if optimizer=='adam':\n",
        "      self.optimizer = tf.optimizers.Adam()\n",
        "    else:\n",
        "      self.optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
        "  \n",
        "  def train(self, x_train=None, y_train=None):\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.vocab_size, self.embedding_dim]))\n",
        "    self.b1 = tf.Variable(tf.random.normal([self.embedding_dim])) #bias\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.embedding_dim, self.vocab_size]))\n",
        "    self.b2 = tf.Variable(tf.random.normal([self.vocab_size]))\n",
        "    for _ in range(self.epochs):\n",
        "      with tf.GradientTape() as t:\n",
        "        #print(x_train, self.W1)\n",
        "        hidden_layer = tf.add(tf.matmul(x_train,self.W1),self.b1)\n",
        "        output_layer = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, self.W2), self.b2))\n",
        "        cross_entropy_loss = tf.reduce_mean(-tf.math.reduce_sum(y_train * tf.math.log(output_layer), axis=[1]))\n",
        "      grads = t.gradient(cross_entropy_loss, [self.W1, self.b1, self.W2, self.b2])\n",
        "      self.optimizer.apply_gradients(zip(grads,[self.W1, self.b1, self.W2, self.b2]))\n",
        "      if(_ % 1000 == 0):\n",
        "        print(cross_entropy_loss)\n",
        "  \n",
        "  def vectorized(self, word_idx):\n",
        "    return (self.W1+self.b1)[word_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "TjPbxrvzCnpR"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "corpus_file = open(\"wiki_corpus.txt\")\n",
        "lines = []\n",
        "i = 0\n",
        "max_num_lines = 10\n",
        "for line in corpus_file:\n",
        "    if i > max_num_lines:\n",
        "        break\n",
        "    \n",
        "    lines.append(line)\n",
        "    i += 1\n",
        "\n",
        "corpus_raw = '.'.join(lines)\n",
        "\n",
        "corpus_raw = corpus_raw.replace(\"'\", \"\")\n",
        "pattern = re.compile('[\\W_-â€“]+')\n",
        "pattern.sub('', corpus_raw)\n",
        "# print(corpus_raw)\n",
        "# convert to lower case\n",
        "corpus_raw = corpus_raw.lower()\n",
        "# raw sentences is a list of sentences.\n",
        "raw_sentences = corpus_raw.split('.')\n",
        "# print(raw_sentences)\n",
        "sentences = []\n",
        "for sentence in raw_sentences:\n",
        "    sentences.append(sentence.split())\n",
        "# print(sentences)\n",
        "#sentences:\n",
        "data = []\n",
        "WINDOW_SIZE = 2\n",
        "for sentence in sentences:\n",
        "    for word_index, word in enumerate(sentence):\n",
        "        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(sentence)) + 1] :\n",
        "            if nb_word != word:\n",
        "                data.append([word, nb_word])\n",
        "words = set()\n",
        "for i in raw_sentences:\n",
        "    for word in i.split():\n",
        "        words.add(word)\n",
        "        \n",
        "word2int = {}\n",
        "int2word = {}\n",
        "vocab_size = len(words) # gives the total number of unique words\n",
        "for i,word in enumerate(words):\n",
        "    word2int[word] = i\n",
        "    int2word[i] = word\n",
        "def to_one_hot(data_point_index, vocab_size):\n",
        "  temp = np.zeros(vocab_size)\n",
        "  temp[data_point_index] = 1\n",
        "  return temp\n",
        "x_train = [] # input word\n",
        "y_train = [] # output word\n",
        "for data_word in data:\n",
        "    x_train.append(to_one_hot(word2int[ data_word[0] ], vocab_size))\n",
        "    y_train.append(to_one_hot(word2int[ data_word[1] ], vocab_size))\n",
        "# convert them to numpy arrays\n",
        "x_train = np.asarray(x_train, dtype='float32')\n",
        "y_train = np.asarray(y_train, dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwaX2D_cDN2c",
        "outputId": "1d66d9d4-d20f-4159-a086-10ba5bb144cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(8.816945, shape=(), dtype=float32)\n",
            "tf.Tensor(2.3829265, shape=(), dtype=float32)\n",
            "tf.Tensor(1.7946837, shape=(), dtype=float32)\n",
            "tf.Tensor(1.4863819, shape=(), dtype=float32)\n",
            "tf.Tensor(1.396186, shape=(), dtype=float32)\n",
            "tf.Tensor(1.3485827, shape=(), dtype=float32)\n",
            "tf.Tensor(1.334331, shape=(), dtype=float32)\n",
            "tf.Tensor(1.330012, shape=(), dtype=float32)\n",
            "tf.Tensor(1.3283578, shape=(), dtype=float32)\n",
            "tf.Tensor(1.3276052, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "cbow = Word2Vec(vocab_size=vocab_size, optimizer='adam', epochs=10000)\n",
        "cbow.train(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3sezRA8Ddrt",
        "outputId": "dbe26f9b-1fba-4189-d185-b47850e7fbbe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
              "array([-5.0990295 ,  1.1574619 , -0.52969515,  1.1401453 , -0.35821998],\n",
              "      dtype=float32)>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cbow.vectorized(word2int['criminal'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uk_sp3yPpqmX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NLP_Baseline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
